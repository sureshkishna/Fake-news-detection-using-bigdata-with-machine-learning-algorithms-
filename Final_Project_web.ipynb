{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI1-eTO-SWj-"
      },
      "source": [
        "## Important packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uBp8eJ5wn4fm"
      },
      "outputs": [],
      "source": [
        "# NLP\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# Pandas is required to read the data.\n",
        "# For some reason pyspark can't read the csv file correctly\n",
        "# So we have to read using pandas and then convert to spark DF\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "from pyspark.ml.feature import IDF, Tokenizer, VectorAssembler\n",
        "from pyspark.ml.feature import StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.sql.functions import when, col, regexp_replace, concat, lit, length\n",
        "from pyspark.sql.types import FloatType, DoubleType\n",
        "from pyspark.ml.classification import NaiveBayesModel, NaiveBayes\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "33jp03OHVhPS"
      },
      "outputs": [],
      "source": [
        "def evaluate(df, labelCol = \"label\", predCol = \"prediction\"):\n",
        "    TP = df.filter((col(labelCol) == 0) & (col(predCol) == 0)).count()\n",
        "    FN = df.filter((col(labelCol) == 1) & (col(predCol) == 0)).count()\n",
        "    FP = df.filter((col(labelCol) == 0) & (col(predCol) == 1)).count()\n",
        "    TN = df.filter((col(labelCol) == 1) & (col(predCol) == 1)).count()\n",
        "\n",
        "    precision = (TP)/(TP+FP)\n",
        "    recall = (TP)/(TP+FN)\n",
        "    print(\"Accuracy: %.3f\" % float((TP+TN)/(TP+TN+FP+FN)))\n",
        "    print(\"Recall: %.3f\" % float(recall))\n",
        "    print(\"Precision: %.3f\" % float(precision))\n",
        "    print(\"F1 Score: %.3f\" % float(2*(precision * recall)/(precision +recall)))\n",
        "\n",
        "    (df\n",
        "        .crosstab('label','prediction')\n",
        "        .withColumnRenamed(\"label_prediction\", \"label\\prediction\")\n",
        "        .orderBy(\"label\\prediction\", asceding = False)\n",
        "        .show()\n",
        "    )\n",
        "\n",
        "    return ([[TP, FP], [FN, TN]], precision, recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_VoOgp5S51X"
      },
      "source": [
        "## Spark Session \\& Reading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SJyOb7_9_uVo"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"my_app_name\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "hghIEiLf0Ai_",
        "outputId": "a6a24524-5fca-49c9-9348-7b8eaac7f8ac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://leofox19:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>my_app_name</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x21f1e1b0ee0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s6boiPBL0AjB"
      },
      "outputs": [],
      "source": [
        "# Load fake news data from CSV into a DataFrame\n",
        "data_path = r\"C:\\Users\\sures\\Downloads\\project\\project\\news.csv\"\n",
        "spark_df = spark.read.csv(data_path, header=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OdMPbCae0AjC"
      },
      "outputs": [],
      "source": [
        "# Remove unimportant rows of the df\n",
        "\n",
        "spark_df = spark_df.filter((spark_df.label == 'FAKE') | (spark_df.label == 'REAL'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Mim3xxQx0AjC"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Assuming 'label' is the name of the column containing the labels\n",
        "string_indexer = StringIndexer(inputCol='label', outputCol='encoded_label')\n",
        "spark_df = string_indexer.fit(spark_df).transform(spark_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3ofwfx60AjD",
        "outputId": "12ac3eda-de8a-4f1e-d113-5b739341dfc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1546"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQhpOKoT0AjD",
        "outputId": "739be14f-7600-4623-d535-2cae7fd58e0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------------+\n",
            "|                 _c0|               title|                text|label| _c4| _c5| _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|_c41|_c42|_c43|_c44|_c45|_c46|_c47|_c48|_c49|_c50|_c51|_c52|_c53|_c54|_c55|_c56|_c57|_c58|_c59|_c60|_c61|_c62|_c63|_c64|_c65|_c66|_c67|_c68|_c69|_c70|_c71|_c72|_c73|_c74|_c75|_c76|_c77|_c78|_c79|_c80|_c81|_c82|_c83|_c84|_c85|_c86|_c87|_c88|_c89|_c90|_c91|_c92|_c93|_c94|_c95|_c96|_c97|_c98|_c99|_c100|_c101|_c102|_c103|_c104|_c105|_c106|_c107|_c108|_c109|_c110|_c111|_c112|_c113|_c114|_c115|_c116|_c117|_c118|_c119|_c120|_c121|_c122|_c123|_c124|_c125|_c126|_c127|_c128|_c129|_c130|_c131|_c132|_c133|_c134|_c135|_c136|_c137|_c138|_c139|_c140|encoded_label|\n",
            "+--------------------+--------------------+--------------------+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------------+\n",
            "|                  95|‘Britain’s Schind...|A Czech stockbrok...| REAL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          1.0|\n",
            "|                1571|Trump takes on Cr...|Killing Obama adm...| REAL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          1.0|\n",
            "|             Oh well| I guess you real...| in that case all...| FAKE|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          0.0|\n",
            "|“I’m going to be ...| I won’t be invol...|         ” he said.\"| REAL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          1.0|\n",
            "|                  So| after tough prim...| both parties are...| REAL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          1.0|\n",
            "|“I am the least r...|” he said at the ...|       believe me.”\"| REAL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          1.0|\n",
            "|                1787|GOP insiders: Car...|On this day in 19...| REAL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          1.0|\n",
            "|                9324|Mike Pence Drapes...|Trump Raises Conc...| FAKE|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          0.0|\n",
            "|                 587|Senate race ranki...|The move would ma...| REAL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          1.0|\n",
            "|— OakTown ☢MAGA O...| 2016 This is a m...| Pug Lover & Game...| FAKE|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          0.0|\n",
            "|This post is part...| an independent b...| a Washington thi...| REAL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          1.0|\n",
            "|                5937|3 Effects of Subs...|Drug and substanc...| FAKE|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          0.0|\n",
            "|                7277|Tree Shaped Verti...|By Amanda Froelic...| FAKE|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          0.0|\n",
            "|                5521|New Comment Featu...|\"Be the First to ...| FAKE|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          0.0|\n",
            "|                9360|Ying and Yang (th...|Ying and Yang (th...| FAKE|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          0.0|\n",
            "|                6646|Police Turn In Ba...|It should be evid...| FAKE|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          0.0|\n",
            "|Despite his lead ...| Trump said he wi...|\"\" he said. \"\"But...| REAL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          1.0|\n",
            "|                1834|Biden makes anoth...|On this day in 19...| REAL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          1.0|\n",
            "|Andrew P. Napolitano| a former judge o...| is the senior ju...| REAL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          1.0|\n",
            "|                6197|Is google and You...|Is google and You...| FAKE|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL| NULL|          0.0|\n",
            "+--------------------+--------------------+--------------------+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "p_Cy2OIJ0AjE"
      },
      "outputs": [],
      "source": [
        "# import libraries for text cleaning\n",
        "\n",
        "from pyspark.sql.functions import isnan, when, count, col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8EMrCun0AjE",
        "outputId": "dee2755a-9168-4486-c252-36715e9c5222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----+----+-----+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------------+\n",
            "|_c0|title|text|label|_c4|_c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|_c41|_c42|_c43|_c44|_c45|_c46|_c47|_c48|_c49|_c50|_c51|_c52|_c53|_c54|_c55|_c56|_c57|_c58|_c59|_c60|_c61|_c62|_c63|_c64|_c65|_c66|_c67|_c68|_c69|_c70|_c71|_c72|_c73|_c74|_c75|_c76|_c77|_c78|_c79|_c80|_c81|_c82|_c83|_c84|_c85|_c86|_c87|_c88|_c89|_c90|_c91|_c92|_c93|_c94|_c95|_c96|_c97|_c98|_c99|_c100|_c101|_c102|_c103|_c104|_c105|_c106|_c107|_c108|_c109|_c110|_c111|_c112|_c113|_c114|_c115|_c116|_c117|_c118|_c119|_c120|_c121|_c122|_c123|_c124|_c125|_c126|_c127|_c128|_c129|_c130|_c131|_c132|_c133|_c134|_c135|_c136|_c137|_c138|_c139|_c140|encoded_label|\n",
            "+---+-----+----+-----+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------------+\n",
            "|  0|    0|   0|    0|  0|  0|  0|  0|  0|  0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|            0|\n",
            "+---+-----+----+-----+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cheking for null values\n",
        "\n",
        "spark_df.select([count(when(isnan(col), col)).alias(col) for col in spark_df.columns]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-KwNJlUTVJj"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtCeamMcTXlS"
      },
      "source": [
        "### Cleaning Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyUnPTcsTdk3",
        "outputId": "bfeae25b-2482-4a9a-9564-30dbe319372d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "906\n",
            "+--------------------+-------------+-----+\n",
            "|           full_text|encoded_label|label|\n",
            "+--------------------+-------------+-----+\n",
            "|Trump Raises Conc...|          0.0|  0.0|\n",
            "|The Manhattan bil...|          1.0|  1.0|\n",
            "| she explains why...|          0.0|  0.0|\n",
            "|Fluoridation of p...|          0.0|  0.0|\n",
            "|YouTube censoring...|          0.0|  0.0|\n",
            "|We obviously spok...|          1.0|  1.0|\n",
            "|Home VIDEO TREY G...|          0.0|  0.0|\n",
            "+--------------------+-------------+-----+\n",
            "only showing top 7 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Deleting all rows that are missing text\n",
        "# df_rmv_nan_text = spark_df.filter(col(\"text\") != \"NaN\")\n",
        "df_rmv_nan_text = spark_df.filter(length(col(\"text\")) > 60)\n",
        "\n",
        "# There are a lot of NaN in the dataset.\n",
        "# Those are Null values in pandas that were\n",
        "# Converted to NaN string in spark df.\n",
        "# Since it is a string, it will not be recognized by na() methods\n",
        "# So, we have to manually change their value:\n",
        "df_no_nan = (df_rmv_nan_text\n",
        "             .withColumn(\"title\", when(col(\"title\") == \"NaN\", \" \")\n",
        "                                            .otherwise(col(\"title\")))\n",
        "             )\n",
        "\n",
        "\n",
        "## NOTE: Later on we will use Tokenizer from PySpark MLlib. This tokenizer\n",
        "##       takes care of converting all characters to lowercase, so it is\n",
        "##       not required in this step.\n",
        "\n",
        "# Remove non-character from title and text\n",
        "df_clean = (df_no_nan\n",
        "\n",
        "                 ## Removing any non-character from title\n",
        "                .withColumn(\"title\",\n",
        "                            regexp_replace(\n",
        "                                col('title'),\n",
        "                                r'[^\\w\\’ ]',''))\n",
        "\n",
        "                ## Removing any non-character from text\n",
        "                .withColumn(\"text\",\n",
        "                            regexp_replace(\n",
        "                                col('text'),\n",
        "                                r'[^\\w\\’ ]',''))\n",
        "\n",
        "                ## Replacing 2 or more whitespaces with 1 whitespace\n",
        "                .withColumn(\"text\",\n",
        "                            regexp_replace(\n",
        "                                col('text'),\n",
        "                                r'[ ]{2,}',' '))\n",
        "\n",
        "                ## Replacing 2 or more whitespaces with 1 whitespace\n",
        "                .withColumn(\"title\",\n",
        "                            regexp_replace(\n",
        "                                col('text'),\n",
        "                                r'[ ]{2,}',' '))\n",
        "                )\n",
        "\n",
        "\n",
        "# Concatenation of title and text when title doesn't appear in text\n",
        "df_combined = (df_clean\n",
        "                    .withColumn('full_text',\n",
        "                                  when(col(\"text\").contains(\n",
        "                                                    concat(col(\"title\"))),\n",
        "                                                    col(\"text\"))\n",
        "\n",
        "                                  .otherwise(concat(col(\"title\"),\n",
        "                                                    lit(\" \"),\n",
        "                                                    col(\"text\"))))\n",
        "                    .select([\"full_text\",\"encoded_label\"])\n",
        "                    .withColumn(\"label\", col(\"encoded_label\").cast(DoubleType()))\n",
        "                    .dropDuplicates()\n",
        "                )\n",
        "\n",
        "\n",
        "# Clean memory\n",
        "del df_rmv_nan_text, df_no_nan, df_clean\n",
        "\n",
        "# Sanity Check\n",
        "print(df_combined.count())\n",
        "df_combined.show(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9YAncjjgHV8"
      },
      "source": [
        "### Check Class Balance\n",
        "\n",
        "Still balanced!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZRUdo-tr9MG",
        "outputId": "1a9ce4c4-8656-48df-bb2e-51e25fca326c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|  0.0|  547|\n",
            "|  1.0|  359|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_combined.groupby(\"label\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkusjNkLmrg5"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf7dYGkSo5IE",
        "outputId": "316fca42-fa4d-4631-e526-5b56d5481311"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "try:\n",
        "    stopwords_ls = stopwords.words('english')\n",
        "except:\n",
        "    nltk.download(\"stopwords\")\n",
        "    stopwords_ls = stopwords.words('english')\n",
        "\n",
        "# Sanity Check\n",
        "stopwords_ls[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38HHWDngVPsm"
      },
      "source": [
        "### Stemmer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vJNIKC-uuSDr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from pyspark import keyword_only\n",
        "from pyspark.ml import Transformer\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
        "\n",
        "\n",
        "class Stemmer(Transformer,\n",
        "                 HasInputCol,\n",
        "                 HasOutputCol,\n",
        "                 DefaultParamsReadable,\n",
        "                 DefaultParamsWritable):\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, inputCol = \"input\", outputCol = \"output\"):\n",
        "        super(Stemmer, self).__init__()\n",
        "        kwargs = self._input_kwargs\n",
        "        self.set_params(**kwargs)\n",
        "\n",
        "    @keyword_only\n",
        "    def set_params(self, inputCol = \"input\", outputCol = \"output\"):\n",
        "        kwargs = self._input_kwargs\n",
        "        self._set(**kwargs)\n",
        "\n",
        "    def get_input_col(self):\n",
        "        return self.getOrDefault(self.inputCol)\n",
        "\n",
        "    def get_output_col(self):\n",
        "        return self.getOrDefault(self.outputCol)\n",
        "\n",
        "    def _transform(self, df):\n",
        "\n",
        "        # Input and output column\n",
        "        input_col = self.get_input_col()\n",
        "        output_col = self.get_output_col()\n",
        "\n",
        "        # Initialize stemmer from nltk package\n",
        "        ps = PorterStemmer()\n",
        "\n",
        "        # User Defined Function: stemming every word in the input column\n",
        "        transform_udf = F.udf(lambda x: [ps.stem(word) for word in x], ArrayType(StringType(), False))\n",
        "\n",
        "        # Return the new df with the new column\n",
        "        return df.withColumn(output_col, transform_udf(input_col))\n",
        "\n",
        "# Sanity check\n",
        "# words = Tokenizer(inputCol=\"text\", outputCol=\"words\").transform(spark_df)\n",
        "# test = Stem(inputCol = \"words\", outputCol = \"test\").transform(words)\n",
        "# test.select([\"words\", \"test\"]).show(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew9QgSIg4xO9"
      },
      "source": [
        "## Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSJKOy1lHgFV"
      },
      "source": [
        "### Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-OcobknvOhr8"
      },
      "outputs": [],
      "source": [
        "# Split data to train and test\n",
        "train, test = df_combined.randomSplit([0.7,0.3], seed=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3L8F2c9N0AjK"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline # pipeline to transform data\n",
        "from pyspark.sql import SparkSession # to initiate spark\n",
        "from pyspark.ml.feature import RegexTokenizer # tokenizer\n",
        "from pyspark.ml.feature import HashingTF, IDF # vectorizer\n",
        "from pyspark.ml.feature import StopWordsRemover # to remove stop words\n",
        "from pyspark.sql.functions import concat_ws, col # to concatinate cols\n",
        "from pyspark.ml.classification import LogisticRegression # ml model\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator # for hyperparameter tuning\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator # to evaluate the model\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics # # performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNwWFlxp0AjK",
        "outputId": "91d0182d-ef3f-41eb-a1b4-7b7608639df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|           full_text|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0| 2006 at 203 am L...|[2006, at, 203, a...|\n",
            "|  0.0| 2016 JohnGHendy ...|[2016, johnghendy...|\n",
            "|  1.0| 2016 The poll ha...|[2016, the, poll,...|\n",
            "|  1.0| 28 states and th...|[28, states, and,...|\n",
            "|  0.0| Allan served a t...|[allan, served, a...|\n",
            "+-----+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# convert sentences to list of words\n",
        "tokenizer = RegexTokenizer(inputCol=\"full_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "train_df = tokenizer.transform(train)\n",
        "train_df.select(['label','full_text', 'words']).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTrcpgxy0AjK",
        "outputId": "fc174265-c982-42db-c8e6-41dbee68e25d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|           full_text|               words|\n",
            "+-----+--------------------+--------------------+\n",
            "|  1.0| 32 patients were...|[32, patients, we...|\n",
            "|  0.0| Brian Dobson is ...|[brian, dobson, i...|\n",
            "|  1.0| Charlie Hebdo ha...|[charlie, hebdo, ...|\n",
            "|  0.0| Churkin stated T...|[churkin, stated,...|\n",
            "|  1.0| Cruz was on cons...|[cruz, was, on, c...|\n",
            "+-----+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# convert sentences to list of words\n",
        "tokenizer = RegexTokenizer(inputCol=\"full_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "\n",
        "test_df = tokenizer.transform(test)\n",
        "test_df.select(['label','full_text', 'words']).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ-91Bel0AjL",
        "outputId": "75276a04-6188-4bfc-e7cc-b1aeb28815c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+\n",
            "|label|           full_text|               words|              filter|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "|  0.0| 2006 at 203 am L...|[2006, at, 203, a...|[2006, 203, link,...|\n",
            "|  0.0| 2016 JohnGHendy ...|[2016, johnghendy...|[2016, johnghendy...|\n",
            "|  1.0| 2016 The poll ha...|[2016, the, poll,...|[2016, poll, marg...|\n",
            "|  1.0| 28 states and th...|[28, states, and,...|[28, states, dist...|\n",
            "|  0.0| Allan served a t...|[allan, served, a...|[allan, served, t...|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filter\")\n",
        "\n",
        "train_df= stopwords_remover.transform(train_df)\n",
        "\n",
        "train_df.select(['label','full_text', 'words', 'filter']).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAzCpq0K0AjL",
        "outputId": "e1aea60d-b1ca-4c58-b6f8-bfce7c07c06f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+\n",
            "|label|           full_text|               words|              filter|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "|  1.0| 32 patients were...|[32, patients, we...|[32, patients, di...|\n",
            "|  0.0| Brian Dobson is ...|[brian, dobson, i...|[brian, dobson, w...|\n",
            "|  1.0| Charlie Hebdo ha...|[charlie, hebdo, ...|[charlie, hebdo, ...|\n",
            "|  0.0| Churkin stated T...|[churkin, stated,...|[churkin, stated,...|\n",
            "|  1.0| Cruz was on cons...|[cruz, was, on, c...|[cruz, constant, ...|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filter\")\n",
        "\n",
        "test_df = stopwords_remover.transform(test_df)\n",
        "\n",
        "test_df.select(['label','full_text', 'words', 'filter']).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxc2dSs-0AjM",
        "outputId": "7dbfb0fc-3064-43ac-bb2d-e4ec23ad1b46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|           full_text|               words|              filter|            features|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|  0.0| 2006 at 203 am L...|[2006, at, 203, a...|[2006, 203, link,...|(10000,[168,3469,...|\n",
            "|  0.0| 2016 JohnGHendy ...|[2016, johnghendy...|[2016, johnghendy...|(10000,[263,671,8...|\n",
            "|  1.0| 2016 The poll ha...|[2016, the, poll,...|[2016, poll, marg...|(10000,[42,808,83...|\n",
            "|  1.0| 28 states and th...|[28, states, and,...|[28, states, dist...|(10000,[120,132,1...|\n",
            "|  0.0| Allan served a t...|[allan, served, a...|[allan, served, t...|(10000,[379,407,6...|\n",
            "|  0.0| American voters ...|[american, voters...|[american, voters...|(10000,[3712,5633...|\n",
            "|  0.0| Bill Clinton is ...|[bill, clinton, i...|[bill, clinton, s...|(10000,[47,157,16...|\n",
            "|  1.0| Brockway said of...|[brockway, said, ...|[brockway, said, ...|(10000,[855,1241,...|\n",
            "|  0.0| Bruce Dixon is m...|[bruce, dixon, is...|[bruce, dixon, ma...|(10000,[551,695,1...|\n",
            "|  1.0| Buffett said a G...|[buffett, said, a...|[buffett, said, g...|(10000,[1287,1780...|\n",
            "|  1.0| CNNsJake Tapper ...|[cnnsjake, tapper...|[cnnsjake, tapper...|(10000,[756,1280,...|\n",
            "|  1.0| Catherine Herrid...|[catherine, herri...|[catherine, herri...|(10000,[1402,1998...|\n",
            "|  0.0| Choose Your Plat...|[choose, your, pl...|[choose, platform...|(10000,[2913,5700...|\n",
            "|  1.0| Clinton said in ...|[clinton, said, i...|[clinton, said, s...|(10000,[756,4320,...|\n",
            "|  1.0| Clinton surrogat...|[clinton, surroga...|[clinton, surroga...|(10000,[756,1011,...|\n",
            "|  0.0| Courage Grows St...|[courage, grows, ...|[courage, grows, ...|(10000,[406,581,6...|\n",
            "|  1.0| DC dedicated to ...|[dc, dedicated, t...|[dc, dedicated, a...|(10000,[310,2432,...|\n",
            "|  1.0| Dan Gallo and Ja...|[dan, gallo, and,...|[dan, gallo, jaso...|(10000,[726,1402,...|\n",
            "|  1.0| David Weigel and...|[david, weigel, a...|[david, weigel, k...|(10000,[585,1402,...|\n",
            "|  1.0| Del The number o...|[del, the, number...|[del, number, pho...|(10000,[1562,1626...|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate term frequency in each article\n",
        "hashing_tf = HashingTF(inputCol=\"filter\", outputCol=\"raw_features\", numFeatures=10000)\n",
        "featurized_data = hashing_tf.transform(train_df)\n",
        "\n",
        "# TF-IDF vectorization of articles\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "idf_vectorizer = idf.fit(featurized_data)\n",
        "train_df = idf_vectorizer.transform(featurized_data)\n",
        "\n",
        "train_df.select(\"label\",'full_text', 'words', 'filter', \"features\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTJ3At7e0AjM",
        "outputId": "455ad7a9-de60-4416-ba39-c049147e6ee2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate term frequency in each article\u001b[39;00m\n\u001b[0;32m      2\u001b[0m hashing_tf \u001b[38;5;241m=\u001b[39m HashingTF(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, numFeatures\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m featurized_data_test \u001b[38;5;241m=\u001b[39m hashing_tf\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mtest_df\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# TF-IDF vectorization of articles\u001b[39;00m\n\u001b[0;32m      6\u001b[0m idf \u001b[38;5;241m=\u001b[39m IDF(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'test_df' is not defined"
          ]
        }
      ],
      "source": [
        "# Calculate term frequency in each article\n",
        "hashing_tf = HashingTF(inputCol=\"filter\", outputCol=\"raw_features\", numFeatures=10000)\n",
        "featurized_data_test = hashing_tf.transform(test_df)\n",
        "\n",
        "# TF-IDF vectorization of articles\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "idf_vectorizer = idf.fit(featurized_data_test)\n",
        "test_df = idf_vectorizer.transform(featurized_data)\n",
        "\n",
        "test_df.select(\"label\",'full_text', 'words', 'filter', \"features\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xynAc2D20AjM"
      },
      "outputs": [],
      "source": [
        "train=train_df.select(\"label\",\"features\")\n",
        "test=test_df.select(\"label\",\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxpsZuKq0AjM",
        "outputId": "de768de2-8063-4dcd-bb48-f4d35aa23f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(10000,[168,3469,...|\n",
            "|  0.0|(10000,[263,671,8...|\n",
            "|  1.0|(10000,[42,808,83...|\n",
            "|  1.0|(10000,[120,132,1...|\n",
            "|  0.0|(10000,[379,407,6...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42JASwR-0AjN",
        "outputId": "116cac01-5bf7-45c5-b39d-bc3e274f0bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(10000,[168,3469,...|\n",
            "|  0.0|(10000,[263,671,8...|\n",
            "|  1.0|(10000,[42,808,83...|\n",
            "|  1.0|(10000,[120,132,1...|\n",
            "|  0.0|(10000,[379,407,6...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rjcdPmWQ0AjN"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train \u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ],
      "source": [
        "train =train.withColumn('label', train['label'].cast('double'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "83nIJoOx0AjO"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=['features'], outputCol='dense_features')\n",
        "train = assembler.transform(train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEOsvq5a0AjO",
        "outputId": "ef949bb9-82ef-498c-de9d-200e673f03df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|            features|      dense_features|\n",
            "+-----+--------------------+--------------------+\n",
            "|  0.0|(10000,[168,3469,...|(10000,[168,3469,...|\n",
            "|  0.0|(10000,[263,671,8...|(10000,[263,671,8...|\n",
            "|  1.0|(10000,[42,808,83...|(10000,[42,808,83...|\n",
            "|  1.0|(10000,[120,132,1...|(10000,[120,132,1...|\n",
            "|  0.0|(10000,[379,407,6...|(10000,[379,407,6...|\n",
            "+-----+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSFIWTem0AjP",
        "outputId": "9dae434c-9bed-48d2-b587-80e6e3650650"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[label: double, dense_features: vector]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.drop(\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "seN8u9te0AjP"
      },
      "outputs": [],
      "source": [
        "from xgboost.spark import SparkXGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz_Jh8I50AjP",
        "outputId": "d51936b2-f915-468a-f44b-a6c1ef9ed95d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-12 13:47:09,044 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
            "\tbooster params: {'objective': 'binary:logistic', 'device': 'cpu', 'nthread': 1}\n",
            "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
            "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
            "2024-05-12 13:47:18,151 INFO XGBoost-PySpark: _fit Finished xgboost training!\n"
          ]
        }
      ],
      "source": [
        "model=SparkXGBClassifier(label_col=\"label\").fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "kbHukJNI0AjR"
      },
      "outputs": [],
      "source": [
        "predict_df=model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONPOVWIK0AjR",
        "outputId": "b1a48b6d-5e08-498f-ff4f-5440298d661b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+----------+--------------------+\n",
            "|label|            features|       rawPrediction|prediction|         probability|\n",
            "+-----+--------------------+--------------------+----------+--------------------+\n",
            "|  0.0|(10000,[168,3469,...|[-0.6931549906730...|       1.0|[0.33333158493041...|\n",
            "|  0.0|(10000,[263,671,8...|[2.95089197158813...|       0.0|[0.95030564069747...|\n",
            "|  1.0|(10000,[42,808,83...|[-0.8122609853744...|       1.0|[0.30740892887115...|\n",
            "|  1.0|(10000,[120,132,1...|[-0.6931549906730...|       1.0|[0.33333158493041...|\n",
            "|  0.0|(10000,[379,407,6...|[1.32028520107269...|       0.0|[0.78922915458679...|\n",
            "|  0.0|(10000,[3712,5633...|[0.72081977128982...|       0.0|[0.67278754711151...|\n",
            "|  0.0|(10000,[47,157,16...|[2.24892854690551...|       0.0|[0.90455806255340...|\n",
            "|  1.0|(10000,[855,1241,...|[-0.3804846704006...|       1.0|[0.40601003170013...|\n",
            "|  0.0|(10000,[551,695,1...|[-1.2882757186889...|       1.0|[0.21614480018615...|\n",
            "|  1.0|(10000,[1287,1780...|[-2.2285096645355...|       1.0|[0.09721934795379...|\n",
            "|  1.0|(10000,[756,1280,...|[-2.8396818637847...|       1.0|[0.05521714687347...|\n",
            "|  1.0|(10000,[1402,1998...|[-1.8213135004043...|       1.0|[0.13927632570266...|\n",
            "|  0.0|(10000,[2913,5700...|[0.30015498399734...|       0.0|[0.57448041439056...|\n",
            "|  1.0|(10000,[756,4320,...|[-3.3191328048706...|       1.0|[0.03492063283920...|\n",
            "|  1.0|(10000,[756,1011,...|[-1.4631378650665...|       1.0|[0.18798786401748...|\n",
            "|  0.0|(10000,[406,581,6...|[-0.4396409094333...|       1.0|[0.39182657003402...|\n",
            "|  1.0|(10000,[310,2432,...|[-0.6931549906730...|       1.0|[0.33333158493041...|\n",
            "|  1.0|(10000,[726,1402,...|[-1.9470452070236...|       1.0|[0.12487590312957...|\n",
            "|  1.0|(10000,[585,1402,...|[-2.2755310535430...|       1.0|[0.09316980838775...|\n",
            "|  1.0|(10000,[1562,1626...|[-1.1665059328079...|       1.0|[0.23748719692230...|\n",
            "+-----+--------------------+--------------------+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predict_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "q11U1Pck0AjS"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJR-q7-R0AjS",
        "outputId": "7be7d3f4-0036-41c7-ce91-a307737d2272"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.9494047619047623\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "auc = evaluator.evaluate(predict_df)\n",
        "\n",
        "# Print the AUC score\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8NZ8fF0AjS"
      },
      "source": [
        "GBT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SgLsIa-T0AjT"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"features_vector\")\n",
        "from pyspark.ml.classification import GBTClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "t4QS2AuZ0AjT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.9442197610751611\n"
          ]
        }
      ],
      "source": [
        "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features_vector\", maxIter=10)\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[assembler, gbt])\n",
        "\n",
        "# Train the GBTClassifier model\n",
        "model_gbt = pipeline.fit(train_df)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions = model_gbt.transform(test_df)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "# Print the AUC score\n",
        "print(\"AUC:\", auc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
